{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# Aim\n","This is a small yet useful kernel for providing an introduction to **Artificial Neural Networks** for people who want to begin their journey into the field of **deep learning**. For this, I have used Keras which is a high-level Neural Networks API built on top of low level neural networks APIs like Tensorflow and Theano. As it is high-level, many things are already taken care of therefore it is easy to work with and a great tool to start with. [Here's the documentation for keras](https://keras.io/)\n","\n","# What is Deep learning?\n","Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost. It is the key to voice control in consumer devices like phones, tablets, TVs, and hands-free speakers. Deep learning is getting lots of attention lately and for good reason. It’s achieving results that were not possible before.\n","\n","\n","# What are artificial neural networks?\n","An artificial neuron network (ANN) is a computational model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN because a neural network changes - or learns, in a sense - based on that input and output. ANNs are considered nonlinear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. ANN is also known as a neural network.\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"source":["<img src=\"https://cdn-images-1.medium.com/max/1000/1*ZX05x1xYgaVoa4Vn2kKS9g.png\">"]},{"cell_type":"markdown","metadata":{"_cell_guid":"17fa9862-de9f-4964-a688-fe9e666f0fd5","_uuid":"5a1835cc73f103d85fe770ff84e0afae99ba692f"},"source":["A single neuron is known as a perceptron. It consists of a layer of inputs(corresponds to columns of a dataframe). Each input has a weight which controls the magnitude of an input.\n","The summation of the products of these input values and weights is fed to the activation function. Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable. \n","\n","They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.  Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer. [Refer to this article for more info.](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)\n","<img src=\"https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/Single-Perceptron.png\">\n","**Concept of backpropagation** - Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks.\n","<img src=\"https://www.researchgate.net/profile/Hassan_Al-Haj_Ibrahim/publication/235338024/figure/fig6/AS:299794191929349@1448487913220/Flow-chart-for-the-back-propagation-BP-learning-algorithm.png\">\n","**Gradient Descent** - To explain Gradient Descent I’ll use the classic mountaineering example. Suppose you are at the top of a mountain, and you have to reach a lake which is at the lowest point of the mountain (a.k.a valley). A twist is that you are blindfolded and you have zero visibility to see where you are headed. So, what approach will you take to reach the lake? The best way is to check the ground near you and observe where the land tends to descend. This will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake. [Refer to this article for more information.](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0de730e5-6631-4d4f-882e-4c34b4a0c37a","_uuid":"e43e4e6f84cb95700e1d05f07f20fb740bcdfd31"},"source":["About Breast Cancer Wisconsin (Diagnostic) Data Set\n","Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n","\n","This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\n","\n","Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n","\n","Attribute Information:\n","\n","1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n","\n","Ten real-valued features are computed for each cell nucleus:\n","\n","a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n","\n","The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n","\n","All feature values are recoded with four significant digits.\n","\n","Missing attribute values: none\n","\n","Class distribution: 357 benign, 212 malignant"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"466b1182-f052-4452-abb5-4fbe3cd92095","_uuid":"6f747b567b4b89f9aaa48e503ddddabf214cb278","collapsed":true,"trusted":true},"outputs":[],"source":["# Importing libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Importing data\n","data = pd.read_csv('data.csv')\n","del data['Unnamed: 32']"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"962b3671-f704-438d-bd54-931767e877cf","_uuid":"f6af18d53dcaabb44c49b50c13ba3dafe7bbae10","collapsed":true,"trusted":true},"outputs":[],"source":["X = data.iloc[:, 2:].values\n","y = data.iloc[:, 1].values\n","\n","# Encoding categorical data\n","from sklearn.preprocessing import LabelEncoder\n","labelencoder_X_1 = LabelEncoder()\n","y = labelencoder_X_1.fit_transform(y)\n","\n","# Splitting the dataset into the Training set and Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n","\n","#Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a0d8f2ca-a51c-44e5-9399-345ec7f36509","_uuid":"d9f057a42b57df5d2ad73b20fc6d81b5e33628e9"},"source":["**Now that we have prepared data, we will import Keras and its packages.**"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"51ab3d86-d1fd-40b6-a463-012e24a5c0a0","_uuid":"c01850e3c738b8a55146cce28e5f6fe82378032e","scrolled":false,"trusted":true},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"5e19dee2-9602-42c9-bc3b-41d912e0aa1d","_uuid":"f07a3637824959ff96269ff3806b994c4f64d117","collapsed":true,"trusted":true},"outputs":[],"source":["# Initialising the ANN\n","classifier = Sequential()"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"b1338309-a776-44b0-b04a-a8bdbf25d10e","_uuid":"cce53ac057db311d8221c027afe2d286bf0a5b51","trusted":true},"outputs":[{"ename":"TypeError","evalue":"('Keyword argument not understood:', 'init')","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Adding the input layer and the first hidden layer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m classifier\u001b[39m.\u001b[39madd(Dense(units\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, init\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39muniform\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m                activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m, input_dim\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m \u001b[39m# Adding dropout to prevent overfitting\u001b[39;00m\n\u001b[0;32m      5\u001b[0m classifier\u001b[39m.\u001b[39madd(Dropout(p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m))\n","File \u001b[1;32mc:\\Users\\29492\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n","File \u001b[1;32mc:\\Users\\29492\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\core\\dense.py:117\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    116\u001b[0m ):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(activity_regularizer\u001b[39m=\u001b[39mactivity_regularizer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(units) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(units, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m units\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[1;32mc:\\Users\\29492\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\29492\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:341\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m allowed_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    331\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    332\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m }\n\u001b[0;32m    340\u001b[0m \u001b[39m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m generic_utils\u001b[39m.\u001b[39;49mvalidate_kwargs(kwargs, allowed_kwargs)\n\u001b[0;32m    343\u001b[0m \u001b[39m# Mutable properties\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[39m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m    347\u001b[0m     \u001b[39misinstance\u001b[39m(trainable, \u001b[39mbool\u001b[39m)\n\u001b[0;32m    348\u001b[0m     \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m     )\n\u001b[0;32m    352\u001b[0m ):\n","File \u001b[1;32mc:\\Users\\29492\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    513\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[1;32m--> 514\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n","\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'init')"]}],"source":["# Adding the input layer and the first hidden layer\n","classifier.add(Dense(units=16, init='uniform',\n","               activation='relu', input_dim=30))\n","# Adding dropout to prevent overfitting\n","classifier.add(Dropout(p=0.1))\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"dd954d81-d773-4e90-aaa3-34d1dcb99e68","_uuid":"3d42817e7446a061dca811ea75ece45b15a4fdd9"},"source":["input_dim - number of columns of the dataset \n","\n","output_dim - number of outputs to be fed to the next layer, if any\n","\n","activation - activation function which is ReLU in this case\n","\n","init - the way in which weights should be provided to an ANN\n"," \n","The **ReLU** function is f(x)=max(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like. One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations. Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is has its own problem, called \"dead neurons,\" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU etc.) can minimize this. Source : [StackExchange](https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d6c2e16-5ffb-43b1-b238-c7434faa8808","_uuid":"6105fb90265fdd6082648004657d55a43b187217","trusted":true},"outputs":[],"source":["# Adding the second hidden layer\n","classifier.add(Dense(output_dim=16, init='uniform', activation='relu'))\n","# Adding dropout to prevent overfitting\n","classifier.add(Dropout(p=0.1))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"628b8221-c76b-40bd-b5b9-b5aff04cbf31","_uuid":"ed51c97f1ea1a10af8bee99604de076b92cb2627","trusted":true},"outputs":[],"source":["# Adding the output layer\n","classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8f80ed07-4a4a-48a4-aa84-b53fc713c61b","_uuid":"71c445f1e371d37d28f4ff405b45d5497ea8bdde"},"source":["output_dim is 1 as we want only 1 output from the final layer.\n","\n","Sigmoid function is used when dealing with classfication problems with 2 types of results.(Submax function is used for 3 or more classification results)\n","<img src=\"https://cdn-images-1.medium.com/max/1000/1*Xu7B5y9gp0iL5ooBj7LtWw.png\">"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fc5ca11-edd6-4ddd-92b0-c5b6866abbec","_uuid":"01ff691f323d264792730ae9d3af72a1e9106271","collapsed":true,"trusted":true},"outputs":[],"source":["# Compiling the ANN\n","classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"6375e2f2-5cf5-42e4-a199-338dbc34a07a","_uuid":"d198bbf1616b2bcd99d138f24559d0eff2a07d01"},"source":["Optimizer is chosen as adam for gradient descent.\n","\n","Binary_crossentropy is the loss function used. \n","\n","Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. [More about this](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"deb09534-714c-4fd2-bac5-d5dff9c474ef","_uuid":"6e2d2d3f17102bc9c4e8f093c914c58412b28ee3","scrolled":true,"trusted":true},"outputs":[],"source":["# Fitting the ANN to the Training set\n","classifier.fit(X_train, y_train, batch_size=100, nb_epoch=150)\n","# Long scroll ahead but worth\n","# The batch size and number of epochs have been set using trial and error. Still looking for more efficient ways. Open to suggestions. "]},{"cell_type":"markdown","metadata":{"_cell_guid":"91cb842e-c693-4ddc-938e-b25dd5fd8d64","_uuid":"e700dd6d93bfc99f1fb10a1914c33764df95b79f"},"source":["Batch size defines number of samples that going to be propagated through the network.\n","\n","An Epoch is a complete pass through all the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc405d00-4e8c-4882-8251-d83d4b269895","_uuid":"2a668b12248656821e4797a95ed0a878a63d8ee1","collapsed":true,"trusted":true},"outputs":[],"source":["# Predicting the Test set results\n","y_pred = classifier.predict(X_test)\n","y_pred = (y_pred > 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22c127bd-8608-4d8e-909b-4b90b7e24fee","_uuid":"b4c8343b16d7022ccb77f8e64f46e96ac5fbe41c","collapsed":true,"trusted":true},"outputs":[],"source":["# Making the Confusion Matrix\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d631e23-de49-42ff-94e3-528f4131e97e","_uuid":"5ed1db854bfc4bc4fb49782e7adcaaf25fe9d41e","trusted":true},"outputs":[],"source":["print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"837036fec6bd2c2ee831bd3232804b001e472678","trusted":true},"outputs":[],"source":["sns.heatmap(cm,annot=True)\n","plt.savefig('h.png')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b9ec3bc5-b073-45e5-aefc-a521c4dc9502","_uuid":"68225ba8a764c54a035a068bf3852f6a084819e0","collapsed":true},"source":["Thanks for reading this. May this help you on your \"deep\" journey into machine learning."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":1}
