4

首先就是相关的背景介绍，其一就是车辆路径问题，通常简称为VRP。VRP本质上涉及优化车辆路线，以便有效地为一组位置或客户提供服务。其目标简单而关键：最小化成本，如燃料消耗和行驶时间，同时最大化服务质量和客户满意度。

然而，在VRP领域存在一个特别复杂的变体，称为合作提货和交付问题，简称为PDP。与传统的VRP不同，PDP涉及多辆车协作完成提货和交付请求的情景。这增加了一层复杂性，因为车辆必须协调行动，以确保资源的有效分配和及时交付。

解决合作PDP的主要挑战之一在于提货和交付配对之间固有的结构依赖关系。这些依赖关系引入了必须仔细考虑的约束条件，以制定有效的路径规划策略。此外，成功应对合作PDP需要不同车辆之间的合作，进一步复杂化了优化过程。

---

6

我将介绍合作PDP的数学建模。合作PDP是一种关键问题，涉及到多个车辆协作完成提货和交付任务。

我们的目标是最小化车辆之间的总行驶距离，以提高运输效率。我们可以用以下数学模型来描述这个问题：

要优化的目标是公式（1）$\mathrm{min} \sum_{k=1}^{K}\sum_{i=0}^{2N}\sum_{j=1}^{2N+1}e_{ij}x_{ijk}$

在这个模型中，$x_{ijk}\in{0,1}$ 表示车辆$k$是否直接从节点$v_i$到节点$v_j$。  $e_{ij}$ 表示空间中的距离。 N是订单的总个数，每个订单包括一个取货结点和送货结点，那么包括出发点一共就有2N+1个，K是智能体的个数。

我们还有以下约束条件：

对于约束条件（2）$\sum_{k=1}^{K}\sum_{j=1}^{2N+1}x_{ijk}=1,\forall i\in[0,2N]$和约束条件（3）$\sum_{k=1}^{K}\sum_{i=0}^{2N}x_{ijk}=1,\forall j\in[1,2N+1]$保证每个结点都会被访问且仅访问一次。

---

7

接下来，就是上面模型的另外3个约束条件，约束条件（4）中的$d_i$表示每个pickup结点的需求量，$C_k$表示智能体k的容量，S表示连续的路由路径序列。这个约束条件保证了智能体的载货量不能超过其自身的容量。约束条件（5）保证了每个配送任务都只能由一个车辆也就是智能体进行配送，不能将一个配送任务拆分成多个交由不同的车辆进行配送，这是它的一个问题的场景设置。约束条件（6）就保证了取货是配送货物的先决条件，这个也是比较容易想清楚的，也就是说必须先到了送货结点对应的取货节点之后才能去送货节点，因为条件2和3保证了每个结点都会被访问且仅被访问一次。

---

9

在方法论部分，我将详细介绍我们的研究方法和框架，以解决合作提货和交付问题（PDP）。

首先，让我们了解状态和动作的解释：

状态：在第$t$步，代理$k$的状态包括其剩余容量$C_k^t$和当前轨迹$S_k^t$。当前位置，即上次访问的节点，用$v_{I_k^t}$表示，其中$I_k^t$是节点索引。 

动作：在第$t$步，车辆代理$k$的动作是确定下一个目标节点，表示为$v(k,t)$。

我们的框架利用这些状态和动作来建立合作PDP的解决方案。

要注意的是在框架中，所有车辆都可以通过中央通信进行沟通，从而确保了在合作PDP环境中的完全可观测性。但是关于通信的细节并没有做过多的限制，相当于都是公开透明的

---

10

在状态转移和奖励解释部分，状态转移就是对于每个代理$k$，在每一步中，$S_k^{t+1}=(S_k^t;{v_{I_k^t}})$，$C_k^{t+1}=C_k^t-d_{I_k^t}$，其中 ; 表示将部分解与新选择的节点连接起来。比较好理解，就是在时间步t+1时路径路由序列添加上新选择的结点，然后再将剩余容量减去需求量。

接着就是奖励机制：该模型的目标是最小化总行驶距离，不会考虑油耗等其他的各种因素什么的。在每一个时间步中，奖励$r_k^t=-e_{I_k^t}$，表示新行驶弧的负长度，所以路径越短奖励值越大，最终的奖励为$R=\sum_{k=1}^{k=K}\sum_{t=0}^{T-1}r_{k}^{t}$，其中$T$是完整回合中的决策步数，是所有个体奖励$r_k^t$的总和。

---

12

接下来，我将介绍MAPDP框架的概述。下图就是框架的总览，其中包括三个大部分，左边的是配对上下文嵌入，就是将原始的输入信息嵌入到向量中，输入信息包括所有结点的坐标和需求。中间的是编码层，编码层是多层叠加的，意思是这个步骤会循环很多次，上层的输出会是下一层的输入。最后输出的结果会输入都协同解码层，解码层进行处理后会输出对应的序列。其中需要注意的是对于所有的智能体来说前两个的结构是共享的，只有最后一个解码模块每个智能体的参数和策略是独立的。下面会依次介绍上面的三个大模块。

---

13

在配对上下文嵌入部分，我们将介绍如何将原始二维位置信息与需求信息结合起来，以生成有效的上下文嵌入。

$\mathcal{L}_i$：原始的二维位置信息。

 $x_i=W^{x}[\mathcal{L_i},d_{i}]+b^{x}$：将这两个特征连接起来，并将它们映射到一个稠密向量xi中。 其中W是一个可训练的参数矩阵，b是一个偏置项。

接着，我们使用以下公式来生成初始的上下文嵌入：

$        h_i^0=\begin{cases}W_0^xx_i+b_0^x,&i=0,\\W_p^x[x_i;x_{i+N}]+b_p^x,&1\le i\le N,\\W_d^xx_i+b_d^x,&N+1\le i\le2N,\end{cases}$

i为0时代表的是出发点，i大于1小于N时表示的是取货结点。i大于N+1小于2N时代表的是送货结点。因为当agent决定是否接受取货请求并前往取货地点vi时，意味着agent随后还必须访问对应的配对投递地点vi+N。因此，一个完整的vi表示应该考虑它的配对传递vi+N的信息。只有当vi已经被访问时，代理才允许访问vi+N。基于vi+N的策略评估不再依赖于vi。这个公式将前述连接的向量映射到更高维度的空间中，并产生了初始的上下文嵌入。（问题建模有疑点）

还有一点就是这里的x和上面问题建模中的x不一样，之前那个x只是用来判断是否有那条路径。

---

14

在上下文编码部分，我们将介绍如何通过多层注意力机制对初始的配对上下文嵌入进行处理。

初始的配对上下文嵌入$h^0_i$将通过$L$个注意力层进行处理，图中这里的每一层虚线相当于一层注意力层。这些注意力层包括：

 多头注意力层（MHA）。 

 跳跃连接层（He等，2016）。

 前馈（FF）层。

 批归一化（BN）层（Ioffe和Szegedy，2015）。 

具体地，我们使用以下公式来实现多头注意力层的处理：
$$
Q_i^h,K_i^h,V_i^h=W_Q^hh_i,W_K^hh_i,W_V^hh_i, \\
				A_i^h=softmax(Q_i^h{K^h}^T/\sqrt{d_k})V_j^h,  \\
				MHA_i=Concat(A_i^1,A_i^2,...,A_i^H)W_O,
$$
对h_i加上可训练的矩阵后得到当前层的Q，K，V。然后将Q和K的转置进行乘积后除以根号d_k，除以根号的目的是让这个更（），然后经过softmax函数后再乘V_j，得到的是一个注意力头的A_i^h，最后将多个注意力头进行链接后乘上一个W_O矩阵得到多头注意力的输出结果。下面用更详细的图来介绍注意力以及多头注意力机制。



---

15

这里主要参考的是attention is all you need里的论文，第一步就是将h_i向量乘上W权重矩阵，得到Q，K，V，如右图输入的是2行4列的矩阵，乘上四行三列的W后得到的是两行三列的Q，K，V。如图，图中的q1，q2分别是Q的第一行和第二行，K，V同理。然后就是这里的第二个公式，对应的就是右下角这张图，Q和K的转置进行矩阵的乘法，这个图很直观的可以看出结果为2*2的矩阵，除以根号d_k后进行softmax操作，softmax就是将矩阵中的值缩放到0-1之间，并且这些值的和为1.最后得到的矩阵再和V矩阵进行矩阵乘法后得到结果就是Z矩阵。左边这张图是一个具体的计算过程，和上面的介绍是一致的。

---

16

下面就是关于多头注意力机制的介绍，这和多卷积核有点类似，多头注意力机制里面就是将多个注意力头分开来计算，每个注意力头的QKV有自己的权重矩阵，权重矩阵是可训练的，上面这个图里面就是有8个注意力头，最后将每个注意力计算得到的结果联结起来，最后再乘上一个输出矩阵，输出矩阵也是一个权重矩阵，用于表征不同注意力头的权重值。

I've been talking 

I've been walking 



---

17

在上下文编码部分，我们将介绍如何通过多层注意力机制对上下文进行编码和处理。

具体地，我们使用以下公式来实现上下文编码的过程：

$        \hat{h_{i}}=BN^{\ell}(h_{i}^{\ell-1}+MHA_{i}^{\ell}(h_{1}^{\ell-1},h_{2}^{\ell-1},\cdots h_{2N}^{\ell-1})),$

就是将h1~2N的编码结果同时作为输入输入到多层注意力层，在通过上面的多头注意力计算算出结果后会经过一个跳跃连接层（He等，2016）。就是图上的这一部分。这个concat&Linear就是上面这个ppt的最后这一部分。输出的结果就是公式中的这一部分，MHA输出加上h_i就是跳跃连接层，add后进行Norm，也就是公式中的BN操作，公式（11）输出的中间量就是h_i^然后公式（12）接着就是对上面这个中间量进行一次feed forward层，最后类似的也是进行一次跳跃连接后进行Batch Normalization。这里输出的结果就会作为输入输入到下一层的Attention层，相对应的就是公式中这里的上标＋1.在经过L层循环后最后将结果输出出去。

---

18

在合作多智能体解码器部分，我们将介绍智能体如何通过合作来进行决策，并记录更新后的状态信息。

我们建立了一个通信层，用于记录不同智能体的更新状态，具体如下：这个Commt就在下面公式（13）中，就是对这k个车辆的信息进行一个汇总

智能体$k$在决策过程中会将关键信息进行串联，包括全局静态表示、当前状态以及其他智能体的信息，如下：

$h_{k,(c)}^t=[\overline{h};h_{I_k^t};C_k^t;Comm^t]$：智能体$k$串联了用于决策的关键信息，包括全局静态表示，全局的静态表示的计算过程在下面，就是一个球平均的过程，然后还有当前状态（当前状态包括当前对应的稠密向量和当前车辆剩余的容量）以及其他智能体的信息。 其中 $v_{I_{k}^{t}}$表示为智能体$k$在第$t$步选择要访问的下一个节点。

---

19

之后就是上面这些虚线框内的内容，每个虚线框内代表是一个智能体的动作，他们都是独立的，在agent Decoder内先是对h1，h2，至h2N进行一次多头注意力机制的计算，计算的过程和上面context encoding层一样，但是这次计算的K值是上一页ppt中的$h_{k,(c)}^t $，经过计算后输出得到g值，然后再会进行一次类似于单头注意力机制的计算，公式14中的$W_{Q,k}$和$W_{K,k}$是最后一次单头注意力机制中的权重矩阵，算出QK后算QK的点积并除以K维度的根号，带入双切正弦函数后乘上剪切率D，D=10是在Bello中提出的一个经验值，是为了更好的探索。公式16计算出u后带入mask掩码后再经过softmax进行缩放。公式17得到的就是一个概率向量，表示的是agent要选择的下一个结点。但是这里面有一个问题就是不同的agent可能在同一个时间步选择相同的下一个访问结点，因为每个agent在生成策略的时候掩码的功能只能保证它解码出的下一个访问结点不和它自身之前已经访问的结点重复，每个mask都是独立分开的。所以fleet handler的作用就是在出现上面问题的时候随机选择一个agent去执行它选择的动作，然后其他的和他冲突的agent在当前时间步就停留在其自己的位置先不动。

---

21

接着就是在随机生成的数据集和真实数据集中对不同的模型进行比较的结果。表格中的N是订单的数量，K是车辆的数量，cost是完成所有订单需要的总路径长度，Gap是与最优模型的差距百分比，time是计算出所有序列的时间，左边的是所有模型的比较，前三个是启发式算法，后三个是强化学习的算法，最后一个就是这篇文章推的MAPDP模型。最后要提出的一点是其他的算法都是解决传统的CVRP问题，这篇文章是通过在算法的解码器部分手动添加额外的pickup-delivery约束掩码来进一步保证输出的可行性的。根据实验结果可以看出来MAPDP模型优于其他的这几个启发式算法和强化学习算法。

---



22

为了进一步研究不同车辆之间在解决合作PDP中的合作，论文还计算了两个案例研究中两个数据集中2N = 50的每辆车的单独行驶距离，如图。此外，由于同时决策可能会遇到与其他代理的冲突，fleet handler被用来处理这种冲突，并保持其他车辆停在原地，论文还计算和报告每个代理k的停车率，就是右边这个halting ratio

我们注意到，数据集对不同Agent工作负载的平衡有很大影响。在所有节点随机分布且集中在单位正方形内的随机数据集中，不同Agent解码器最终会有相似的工作负载。然而，在节点分布不平衡甚至有些节点位置较远的真实数据集中，Agent表现出显著差异。这是因为，在同一决策步骤中，一些车辆可能会被分配一个距离较远的节点，这大大增加了它们的总旅行距离。其中旅行距离最长的智能体旅行了212个空间单位，而轨迹最短的智能体只旅行了103个单位。同时，在两个数据集中，证明了被暂停最多的代理最终具有最低的工作负载。这是因为这样的代理跳过决策步骤，而其他代理继续在实例图上移动。

---

23

论文还进一步研究MAPDP设计的每个部分的有效性，如图所示。MAPDP的两个附加变体按照相同的评估协议进行训练和评估：MAPDP—SP代表简化模型，其中所有代理解码器共享相同的参数。因此，所有代理变得同质以生成单独的解决方案。MAPDP—NC代表多个Agent框架不考虑通信嵌入，只根据Agent自身状态和全局图嵌入生成上下文嵌入。

结果表明，两种MAPDP变体的性能都劣于原始MAPDP。MAPDP-SP表明异构训练可以基于纯参数共享进一步略微提高其有效性。然而，MAPDP-NC甚至劣于许多其他基线。这是因为，在一个完全合作的场景中，与其他代理的最新通信对于有效协调至关重要。

---

24

在本研究中，我们提出了MAPDP框架，利用多智能体强化学习技术有效解决了合作式提取和送货问题（PDP），通过捕捉依赖关系并促进多个车辆之间的合作，MAPDP框架显著提高了PDP问题的解决效率。

MAPDP在所有实验设置中至少优于现有基线1.64%，展现了其在生成PDP高质量解决方案方面的卓越性能。

MAPDP框架的成功得益于集中式MARL框架、配对上下文嵌入和合作解码器，这些技术共同克服了PDP的挑战。

未来的研究方向可能包括探索MAPDP在更大问题实例中的可扩展性，结合实时约束条件，并将框架调整到动态环境中。